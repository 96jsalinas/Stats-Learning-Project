---
title: "Credit Card Default: EDA & Preprocessing"
format:
    html:
        code-fold: true
        toc: true
        toc-depth: 3
jupyter: python3
---

# Overview

This report analyses the **UCI Default of Credit Card Clients** dataset (30 000 Taiwanese cardholders, 2005). The goal is to predict whether a client will default on their payment in October 2005. 

---

# 1. Load & Basic Cleaning

We first load the dataset from a local cache (or download it if absent), drop the meaningless `ID` column, and rename every feature to a more readable snake-case convention. Two helper label columns (`edu_label`, `gender_label`) are created **for visualisation only** and will be dropped before modelling.

```{python}
import os
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import skew
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

local_file = "default_credit_card_clients.xlsx"
url = ("https://archive.ics.uci.edu/ml/machine-learning-databases"
       "/00350/default%20of%20credit%20card%20clients.xls")

if os.path.exists(local_file):
    df = pd.read_excel(local_file)
else:
    df = pd.read_excel(url, header=1)
    df.to_excel(local_file, index=False)

rename_dict = {
    'LIMIT_BAL': 'credit_limit', 'SEX': 'gender',
    'EDUCATION': 'education', 'MARRIAGE': 'marital_status', 'AGE': 'age',
    'PAY_0': 'status_sep', 'PAY_2': 'status_aug', 'PAY_3': 'status_jul',
    'PAY_4': 'status_jun', 'PAY_5': 'status_may', 'PAY_6': 'status_apr',
    'BILL_AMT1': 'bill_sep', 'BILL_AMT2': 'bill_aug', 'BILL_AMT3': 'bill_jul',
    'BILL_AMT4': 'bill_jun', 'BILL_AMT5': 'bill_may', 'BILL_AMT6': 'bill_apr',
    'PAY_AMT1': 'paid_sep', 'PAY_AMT2': 'paid_aug', 'PAY_AMT3': 'paid_jul',
    'PAY_AMT4': 'paid_jun', 'PAY_AMT5': 'paid_may', 'PAY_AMT6': 'paid_apr',
    'default payment next month': 'default'
}
df.rename(columns=rename_dict, inplace=True)
if 'ID' in df.columns:
    df.drop('ID', axis=1, inplace=True)

# EDA-only label columns
df['edu_label']    = df['education'].map({1: 'Grad', 2: 'Uni', 3: 'HS',
                                           4: 'Other', 5: 'Other',
                                           6: 'Other', 0: 'Other'})
df['gender_label'] = df['gender'].map({1: 'Male', 2: 'Female'})

print(f"Shape: {df.shape}")
df.head(3)
```

The dataset contains **30 000 rows** and **25 feature columns** after renaming. No structural issues (duplicate headers, merged cells) were found in the raw Excel file.

---

# 2. Data Integrity & Summary

Before any modelling we perform a per-column audit covering: data type, count of missing values, mean, skewness and excess kurtosis.

```{python}
def academic_summary(df_in):
    numeric_df = df_in.select_dtypes(include=[np.number])
    summary = pd.DataFrame(index=numeric_df.columns)
    summary['Type']     = numeric_df.dtypes
    summary['Missing']  = numeric_df.isnull().sum()
    summary['Mean']     = numeric_df.mean().round(2)
    summary['Skewness'] = numeric_df.skew().round(2)
    summary['Kurtosis'] = numeric_df.kurt().round(2)
    return summary

academic_summary(df)
```

**Key findings:**

* **No missing values**: the dataset is complete, so no imputation is required.
* **Monetary columns** (`bill_*`, `paid_*`, `credit_limit`) show strong positive skewness (typically > 2), indicating a long right tail consistent with a small number of very high-spending or high-limit clients. These columns will benefit from a log transform.
* **Repayment status columns** (`status_*`) are encoded as integers (−2 to 8). They are ordinal, not continuous, and will be treated accordingly.
* **Target (`default`)** is binary (0 / 1).

---

# 3. Visual EDA

## 3.1 Univariate Analysis

We inspect the distribution of the target variable, age, credit limit, and education level.

```{python}
fig, axes = plt.subplots(2, 2, figsize=(14, 9))

sns.countplot(x='default', data=df, ax=axes[0, 0], palette='viridis')
axes[0, 0].set_title('Target Balance (Default vs Non-Default)')
axes[0, 0].set_xlabel('Default (0 = No, 1 = Yes)')

sns.histplot(df['age'], bins=30, kde=True, ax=axes[0, 1], color='skyblue')
axes[0, 1].set_title('Age Distribution')

sns.histplot(df['credit_limit'], bins=30, kde=True, ax=axes[1, 0], color='salmon')
axes[1, 0].set_title('Credit Limit Distribution')

sns.countplot(x='edu_label', data=df, ax=axes[1, 1], palette='pastel',
              order=['Grad', 'Uni', 'HS', 'Other'])
axes[1, 1].set_title('Education Background')

plt.tight_layout()
plt.show()
```

**Interpretations:**

* **Class imbalance**: roughly 78 % of clients did *not* default vs. 22 % who did. This imbalance is moderate, it will be accounted for in modelling but is not severe enough to require aggressive resampling on its own.
* **Age**: ages range from 21 to 79, with a right-skewed distribution peaking around 26–30. Most clients are young adults, the long right tail means a handful of elderly clients hold much higher credit limits on average.
* **Credit limit**: highly right-skewed: most clients cluster at lower limits (NT\$50 000–200 000) with a few outliers exceeding NT\$800 000. Log transformation will be applied before modelling.
* **Education**: university graduates form the largest group, followed by graduate-school cardholders. The small "Other/Unknown" category (codes 0, 5, 6) will be collapsed into a single level.

---

## 3.2 Bivariate Analysis: Features vs Default

```{python}
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

sns.boxplot(x='default', y='age', data=df, palette='Set2', ax=axes[0])
axes[0].set_title('Age vs Default Status')
axes[0].set_xlabel('Default (0 = No, 1 = Yes)')

sns.boxplot(x='default', y='credit_limit', data=df, palette='Set2', ax=axes[1])
axes[1].set_title('Credit Limit vs Default Status')
axes[1].set_xlabel('Default (0 = No, 1 = Yes)')

plt.tight_layout()
plt.show()
```

**Interpretations:**

* **Age**: the median age is very similar across both groups (around 35), suggesting age alone is a weak discriminator. The interquartile ranges overlap heavily, any predictive power age carries is likely captured through interaction with other variables.
* **Credit limit**: defaulters tend to hold *lower* credit limits than non-defaulters. This makes economic sense: issuers typically grant higher limits to clients with a stronger repayment history. The difference is meaningful but distributions still overlap substantially, so credit limit will be one signal among many rather than a dominant predictor.

---

## 3.3 Multicollinearity Assessment

High correlations between predictors can distort probabilistic models (e.g. Naïve Bayes) and inflate variance in regression-based approaches. We focus on the six monthly bill-amount columns.

```{python}
bill_cols = [c for c in df.columns if 'bill' in c]

plt.figure(figsize=(9, 6))
sns.heatmap(df[bill_cols].corr(), annot=True, cmap='viridis', fmt=".2f",
            linewidths=0.5)
plt.title("Pairwise Correlations: Monthly Bill Amounts")
plt.tight_layout()
plt.show()
```

**Interpretations:**

* Adjacent months (e.g. `bill_sep` and `bill_aug`) show very high correlations (around 0.90–0.95), which is expected: a client's outstanding balance carries over month to month.
* Correlations weaken with temporal distance : `bill_sep` and `bill_apr` are still moderately correlated (around 0.70), but less so than neighbouring months.
* This near-multicollinearity is a known challenge for Naïve Bayes (which assumes feature independence) and for LDA (which inverts the covariance matrix). Dimensionality reduction or careful feature selection will be considered during modelling.

---

## 3.4 Temporal Trends: Bills vs Payments

We plot the mean bill amount and mean payment amount across the six observation months (April–September 2005) to detect any macro-level trend.

```{python}
bill_cols_ordered = ['bill_apr', 'bill_may', 'bill_jun',
                     'bill_jul', 'bill_aug', 'bill_sep']
paid_cols_ordered = ['paid_apr', 'paid_may', 'paid_jun',
                     'paid_jul', 'paid_aug', 'paid_sep']
months = ['Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep']

plt.figure(figsize=(10, 5))
plt.plot(months, df[bill_cols_ordered].mean(), marker='o', label='Avg Bill Amount')
plt.plot(months, df[paid_cols_ordered].mean(), marker='s', label='Avg Amount Paid')
plt.title('Average Bill vs Payment Over Time (Apr–Sep 2005)')
plt.ylabel('NT$ (average across all clients)')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

**Interpretations:**

* Average outstanding bill amounts remain **consistently higher** than average payments across all months, confirming that most clients carry a revolving balance.
* The gap between billed and paid amounts is relatively stable, suggesting no dramatic macro shock during this period.
* Both series are fairly flat month-to-month, indicating the dataset captures a steady-state spending regime rather than a seasonal spike. The mismatch between billed and paid amounts is a structural feature that repayment-status variables will encode more directly.

---

# 4. Preprocessing Pipeline

## 4.1 Variable Grouping

We explicitly categorise every feature before applying transformations, ensuring the right treatment for each variable type.

```{python}
monetary_cols = [
    'credit_limit', 'age',
    'bill_sep', 'bill_aug', 'bill_jul', 'bill_jun', 'bill_may', 'bill_apr',
    'paid_sep', 'paid_aug', 'paid_jul', 'paid_jun', 'paid_may', 'paid_apr'
]
ordinal_cols = [
    'education',
    'status_sep', 'status_aug', 'status_jul',
    'status_jun', 'status_may', 'status_apr'
]
nominal_cols = ['gender', 'marital_status']

print("Monetary (continuous / skewed):", monetary_cols)
print("\nOrdinal (integer-coded):", ordinal_cols)
print("\nNominal (to one-hot encode):", nominal_cols)
```

---

## 4.2 Cleaning & Encoding

Undocumented category codes (0, 5, 6 in `education`; 0 in `marital_status`) are collapsed into the existing "Other" category. Nominal variables are one-hot encoded (dropping the first level to avoid perfect multicollinearity).

```{python}
df['education']      = df['education'].replace([0, 5, 6], 4)
df['marital_status'] = df['marital_status'].replace(0, 3)

df_encoded = pd.get_dummies(df, columns=nominal_cols, drop_first=True)
X = df_encoded.drop(columns=['default', 'edu_label', 'gender_label'], errors='ignore')
y = df_encoded['default']

print(f"Feature matrix shape: {X.shape}")
print("Columns:", list(X.columns))
```

---

## 4.3 Train / Test Split

A stratified 70 / 30 split preserves the original class ratio in both subsets, preventing optimistic evaluation bias from accidental over-representation of non-defaults in the test set.

```{python}
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

print(f"Training set : {X_train.shape[0]} rows  "
      f"(default rate = {y_train.mean():.3f})")
print(f"Test set     : {X_test.shape[0]} rows  "
      f"(default rate = {y_test.mean():.3f})")
```

The near-identical default rates in both splits confirm that stratification worked correctly.

---

## 4.4 Selective Log Transformation

To reduce skewness in monetary columns we apply **log1p** (i.e. log(1 + x)) but only to columns whose absolute skewness on the training set exceeds a threshold of 0.75. This threshold-based approach avoids unnecessarily transforming variables that are already approximately symmetric, and the threshold decision is made on training data only to prevent data leakage.

```{python}
SKEW_THRESHOLD = 0.75
logged_cols = []
train_skewness_before = X_train[monetary_cols].skew()

for col in monetary_cols:
    if abs(train_skewness_before[col]) > SKEW_THRESHOLD:
        X_train[col] = np.log1p(X_train[col].clip(lower=0))
        X_test[col]  = np.log1p(X_test[col].clip(lower=0))
        logged_cols.append(col)

print(f"Columns log-transformed ({len(logged_cols)}): {logged_cols}")
```

---

## 4.5 Standardisation

Finally, all features are standardised to zero mean and unit variance using `StandardScaler` fitted **exclusively on the training set**. The same parameters are then applied to the test set, again avoiding leakage.

```{python}
final_cols = (monetary_cols + ordinal_cols +
              [c for c in X.columns if 'gender_' in c or 'marital_status_' in c])

X_train = X_train[final_cols]
X_test  = X_test[final_cols]

scaler = StandardScaler()
X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)
X_test_scaled  = pd.DataFrame(scaler.transform(X_test),  columns=X_test.columns)

print(f"Final feature count: {X_train_scaled.shape[1]}")
print("Sample of scaled training data:")
X_train_scaled.head(3)
```

Standardisation is essential for distance-based and probabilistic models (LDA, Naïve Bayes, logistic regression with regularisation) that are sensitive to feature scale.

---

# 5. Preprocessing Verification

We confirm that log transformation meaningfully reduced skewness in the targeted columns.

```{python}
if logged_cols:
    comparison = pd.DataFrame({
        'Skew Before': train_skewness_before[logged_cols].values,
        'Skew After':  X_train[logged_cols].skew().values
    }, index=logged_cols)
    comparison['Reduction %'] = (
        (comparison['Skew Before'].abs() - comparison['Skew After'].abs())
        / comparison['Skew Before'].abs() * 100
    ).round(1)
    display(comparison.round(3))
```

**Interpretation:** For all transformed columns, absolute skewness dropped substantially (typically from > 2 to < 1), bringing the distributions much closer to symmetry. This will improve the Gaussian assumptions underlying LDA and Naïve Bayes and reduce the influence of outliers on distance-based models.

---

# 6. Discriminant & Probabilistic Classifiers

## 6.1 Overview

We evaluate three generative classifiers that share a common probabilistic framework: they all model $P(\mathbf{x} \mid y)$ and apply Bayes' theorem to obtain the posterior $P(y \mid \mathbf{x})$.

| Model | Covariance structure | Decision boundary |
|---|---|---|
| **LDA** | Shared $\Sigma$ across classes | Linear |
| **QDA** | Per-class $\Sigma_k$ | Quadratic |
| **Gaussian Naïve Bayes** | Diagonal (feature independence) | Quadratic |

```{python}
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import (
    classification_report, confusion_matrix, roc_auc_score,
    RocCurveDisplay, ConfusionMatrixDisplay
)
import warnings
warnings.filterwarnings('ignore')

# Reset indices so everything aligns cleanly
X_train_scaled = X_train_scaled.reset_index(drop=True)
X_test_scaled  = X_test_scaled.reset_index(drop=True)
y_train = y_train.reset_index(drop=True)
y_test  = y_test.reset_index(drop=True)
```

---

## 6.2 Linear Discriminant Analysis (LDA)

### Theory

LDA assumes each class $k$ follows a multivariate Gaussian $\mathcal{N}(\boldsymbol{\mu}_k, \Sigma)$ with a **shared** covariance matrix $\Sigma$. This shared-covariance assumption makes the log-posterior ratio linear in $\mathbf{x}$, yielding linear decision boundaries. Parameters are estimated by maximum likelihood: class means $\hat{\boldsymbol{\mu}}_k$ and a pooled within-class scatter matrix.

**Key assumptions:**

* Features are Gaussian within each class.
* All classes share the same covariance structure.

### Fit & Evaluate

```{python}
lda = LinearDiscriminantAnalysis()
lda.fit(X_train_scaled, y_train)

y_pred_lda = lda.predict(X_test_scaled)
y_prob_lda = lda.predict_proba(X_test_scaled)[:, 1]
auc_lda    = roc_auc_score(y_test, y_prob_lda)

print("=== LDA: Classification Report ===")
print(classification_report(y_test, y_pred_lda, target_names=['No Default', 'Default']))
print(f"ROC-AUC: {auc_lda:.4f}")
```

```{python}
fig, axes = plt.subplots(1, 2, figsize=(13, 4))

ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_lda),
                       display_labels=['No Default', 'Default']).plot(ax=axes[0], colorbar=False)
axes[0].set_title('LDA - Confusion Matrix')

RocCurveDisplay.from_predictions(y_test, y_prob_lda, ax=axes[1],
                                 name=f'LDA (AUC = {auc_lda:.3f})')
axes[1].plot([0, 1], [0, 1], 'k--', lw=1)
axes[1].set_title('LDA - ROC Curve')
plt.tight_layout()
plt.show()
```

### Interpretation

* LDA achieves **81 % overall accuracy**, however recall on non-defaulters is 96 % while recall on defaulters is only **27 %**, meaning nearly three quarters of actual defaulters are missed.
* The **ROC-AUC of 0.74** is the more meaningful summary: it confirms that LDA's linear projection captures genuine discriminative signal, ranking a randomly chosen defaulter above a randomly chosen non-defaulter 74 % of the time.
* The precision / recall trade-off for the default class (precision 0.65, recall 0.27, F1 0.38) reflects the shared-covariance assumption pulling the decision boundary toward the majority class. Lowering the classification threshold from 0.5 would improve recall at the cost of precision.

---

## 6.3 Quadratic Discriminant Analysis (QDA)

### Theory

QDA relaxes LDA's shared-covariance assumption by estimating a **separate** covariance matrix $\Sigma_k$ for each class. The log-posterior then contains a quadratic term in $\mathbf{x}$, producing curved decision boundaries. This gives QDA more flexibility but at the cost of estimating many more parameters ($p(p+1)/2$ per class), which can hurt performance when $n$ is small or features are highly correlated.

**Key assumptions:**

* Features are Gaussian within each class.
* Classes may have different covariance structures.

### Fit & Evaluate

```{python}
qda = QuadraticDiscriminantAnalysis(reg_param=0.01)
qda.fit(X_train_scaled, y_train)

y_pred_qda = qda.predict(X_test_scaled)
y_prob_qda = qda.predict_proba(X_test_scaled)[:, 1]
auc_qda    = roc_auc_score(y_test, y_prob_qda)

print("=== QDA: Classification Report ===")
print(classification_report(y_test, y_pred_qda, target_names=['No Default', 'Default']))
print(f"ROC-AUC: {auc_qda:.4f}")
```

> `reg_param=0.01` adds a small ridge to each class covariance matrix, stabilising inversion when some features are near-collinear (as seen in the bill-amount correlations from Section 3.3).

```{python}
fig, axes = plt.subplots(1, 2, figsize=(13, 4))

ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_qda),
                       display_labels=['No Default', 'Default']).plot(ax=axes[0], colorbar=False)
axes[0].set_title('QDA: Confusion Matrix')

RocCurveDisplay.from_predictions(y_test, y_prob_qda, ax=axes[1],
                                 name=f'QDA (AUC = {auc_qda:.3f})')
axes[1].plot([0, 1], [0, 1], 'k--', lw=1)
axes[1].set_title('QDA: ROC Curve')
plt.tight_layout()
plt.show()
```

### Interpretation

* QDA sacrifices overall accuracy (75 % vs LDA's 81 %) in exchange for much better balance on the default class: recall rises from 27 % (LDA) to **49 %**, and F1 improves from 0.38 to **0.47**. This reflects the quadratic boundary's ability to enclose the default cluster more tightly.
* However, QDA's **ROC-AUC of 0.72 is the lowest of the three models**, below LDA (0.74) and Naïve Bayes (0.74). The multicollinearity in the bill columns (Section 3.3) makes per-class covariance estimation unreliable.
* The two classes do appear to differ in their covariance structure (defaulters show more volatile spending), which is why QDA's recall gain over LDA is real, but the correlated features limit how much the extra flexibility can be exploited.

---

## 6.4 Gaussian Naïve Bayes

### Theory

Gaussian Naïve Bayes (GNB) takes the independence assumption to its extreme: given the class label, every feature is treated as **conditionally independent**, so the class-conditional density factorises as:

$$P(\mathbf{x} \mid y = k) = \prod_{j=1}^{p} \mathcal{N}(x_j \mid \mu_{kj},\, \sigma_{kj}^2)$$

This makes GNB equivalent to QDA with diagonal (per-class) covariance matrices. Parameter estimation is trivially fast and the model is robust to high dimensionality, but the independence assumption is strongly violated here (see the bill-amount correlations in Section 3).

**Key assumptions:**

* Features are conditionally independent given the class label.
* Each feature is Gaussian within each class.

### Fit & Evaluate

```{python}
gnb = GaussianNB()
gnb.fit(X_train_scaled, y_train)

y_pred_gnb = gnb.predict(X_test_scaled)
y_prob_gnb = gnb.predict_proba(X_test_scaled)[:, 1]
auc_gnb    = roc_auc_score(y_test, y_prob_gnb)

print("=== Gaussian Naïve Bayes: Classification Report ===")
print(classification_report(y_test, y_pred_gnb, target_names=['No Default', 'Default']))
print(f"ROC-AUC: {auc_gnb:.4f}")
```

```{python}
fig, axes = plt.subplots(1, 2, figsize=(13, 4))

ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_gnb),
                       display_labels=['No Default', 'Default']).plot(ax=axes[0], colorbar=False)
axes[0].set_title('Naïve Bayes: Confusion Matrix')

RocCurveDisplay.from_predictions(y_test, y_prob_gnb, ax=axes[1],
                                 name=f'Naïve Bayes (AUC = {auc_gnb:.3f})')
axes[1].plot([0, 1], [0, 1], 'k--', lw=1)
axes[1].set_title('Naïve Bayes: ROC Curve')
plt.tight_layout()
plt.show()
```

### Interpretation

* Despite the strongly violated independence assumption, GNB achieves a **ROC-AUC of 0.74**, matching LDA, this is probably because the ranking of predicted probabilities is still informative even when absolute probability values are miscalibrated.
* GNB has the **highest recall on defaulters (51 %)** and a competitive F1 of 0.49, outperforming both LDA (F1 0.38) and QDA (F1 0.47) on the minority class at the cost of lower overall accuracy (76 %).
* The precision on defaults is only 0.47 (vs LDA's 0.65), confirming the miscalibration: GNB flags more clients as risky than warranted.

---

## 6.5 Model Comparison

### Combined ROC Curves

```{python}
fig, ax = plt.subplots(figsize=(8, 6))

for name, probs in [('LDA', y_prob_lda), ('QDA', y_prob_qda), ('Naïve Bayes', y_prob_gnb)]:
    RocCurveDisplay.from_predictions(y_test, probs, ax=ax,
                                     name=f'{name} (AUC = {roc_auc_score(y_test, probs):.3f})')

ax.plot([0, 1], [0, 1], 'k--', lw=1, label='Random classifier')
ax.set_title('ROC Curves: LDA vs QDA vs Naïve Bayes')
ax.legend(loc='lower right')
plt.tight_layout()
plt.show()
```

### Summary Table

```{python}
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

rows = []
for name, y_pred, y_prob in [
    ('LDA',         y_pred_lda, y_prob_lda),
    ('QDA',         y_pred_qda, y_prob_qda),
    ('Naïve Bayes', y_pred_gnb, y_prob_gnb),
]:
    rows.append({
        'Model':                  name,
        'Accuracy':               accuracy_score(y_test, y_pred),
        'Precision (Default)':    precision_score(y_test, y_pred),
        'Recall (Default)':       recall_score(y_test, y_pred),
        'F1 (Default)':           f1_score(y_test, y_pred),
        'ROC-AUC':                roc_auc_score(y_test, y_prob),
    })

summary_df = pd.DataFrame(rows).set_index('Model').round(4)
display(summary_df)
```

### Discussion

* **LDA** achieves the highest overall accuracy (81 %) and highest ROC-AUC (0.74), making it the best ranking model. Its weakness is the very low recall on defaulters (27 %): it is a conservative model that only flags cases it is highly confident about, producing a precision of 0.65 on the default class.
* **QDA** has the lowest accuracy (75 %) and lowest AUC (0.72), but nearly doubles LDA's recall on defaults (49 %). The extra covariance flexibility partially captures the different spending volatility between the two classes, though multicollinearity in the bill features caps the gains.
* **Gaussian Naïve Bayes** strikes the best minority-class balance: highest default recall (51 %), best default F1 (0.49), and an AUC (0.74) that matches LDA. It is the best choice when catching defaulters matters more than precision, provided the output probabilities are recalibrated before use.
* The core issue of all three models is the **precision–recall trade-off on the minority class**, none reaches a default F1 above 0.49.

---

# 7. Cost-Sensitive Learning

## 7.1 The Cost of Ignoring Imbalance

The dataset has a 22 % default rate , predicting "no default" for every observation would yield 78 % accuracy yet catch zero defaulters. More importantly, **the two types of error are not equally costly** in credit risk:

* **False Negative** (missing a defaulter): the bank extends credit it will not recover, high financial loss.
* **False Positive** (flagging a good client): a brief manual review is triggered, low administrative cost.

This asymmetry must be encoded explicitly rather than left to the default 0.5 threshold.

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    confusion_matrix, ConfusionMatrixDisplay,
    classification_report, roc_auc_score,
    precision_score, recall_score, f1_score, accuracy_score
)
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB

# Naive baseline: always predict majority class
y_always_no_default = np.zeros(len(y_test), dtype=int)
print("Naive 'always No-Default' baseline")
print(classification_report(y_test, y_always_no_default,
                             target_names=['No Default', 'Default']))
```

---

## 7.2 Defining a Cost Matrix

We assign costs reflecting the business reality of a credit card issuer.

| | **Predicted: No Default** | **Predicted: Default** |
|---|---|---|
| **Actual: No Default** | 0 (TN) | 1 (FP unnecessary review) |
| **Actual: Default** | 5 (FN unrecovered credit) | 0 (TP) |

A missed defaulter costs **5×** more than a false alarm. This yields the theoretical optimal threshold:

$$p^* = \frac{c_{FP}}{c_{FP} + c_{FN}} = \frac{1}{1 + 5} \approx 0.167$$

Any client whose predicted default probability exceeds 0.167 should be classified as a defaulter.

```{python}
# Cost matrix: rows = actual, cols = predicted
#              No Default  Default
C = np.array([[0,          1],    # Actual: No Default
              [5,          0]])   # Actual: Default

c_fp = C[0, 1]   # FP cost
c_fn = C[1, 0]   # FN cost
p_star = c_fp / (c_fp + c_fn)
print(f"Theoretical cost-optimal threshold: p* = {p_star:.4f}")

def expected_cost(y_true, y_pred, cost_matrix, normalize=True):
    cm = confusion_matrix(y_true, y_pred)
    total = np.sum(cm * cost_matrix)
    return total / len(y_true) if normalize else total
```

---

## 7.3 Solution 1: Decision Threshold Tuning

The LDA model already yields calibrated probabilities (AUC 0.74). Instead of always predicting the class with the highest probability, we apply a custom threshold derived from the cost matrix.

### Threshold sweep

```{python}
thresholds = np.arange(0.05, 0.70, 0.025)
rows_thresh = []

for t in thresholds:
    y_pred_t = (y_prob_lda >= t).astype(int)
    rows_thresh.append({
        'Threshold':          t,
        'Accuracy':           accuracy_score(y_test, y_pred_t),
        'Recall (Default)':   recall_score(y_test, y_pred_t, zero_division=0),
        'Precision (Default)':precision_score(y_test, y_pred_t, zero_division=0),
        'F1 (Default)':       f1_score(y_test, y_pred_t, zero_division=0),
        'Expected Cost':      expected_cost(y_test, y_pred_t, C),
    })

thresh_df = pd.DataFrame(rows_thresh)

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

for metric, color in [('Recall (Default)', 'tab:orange'),
                       ('Precision (Default)', 'tab:blue'),
                       ('F1 (Default)', 'tab:green'),
                       ('Accuracy', 'tab:gray')]:
    axes[0].plot(thresh_df['Threshold'], thresh_df[metric], label=metric, color=color)
axes[0].axvline(p_star, color='red', linestyle='--', label=f'p* = {p_star:.2f}')
axes[0].set_title('LDA: Metrics vs Decision Threshold')
axes[0].set_xlabel('Threshold')
axes[0].legend(fontsize=8)
axes[0].grid(True, alpha=0.3)

axes[1].plot(thresh_df['Threshold'], thresh_df['Expected Cost'], color='crimson')
axes[1].axvline(p_star, color='red', linestyle='--', label=f'p* = {p_star:.2f}')
opt_idx = thresh_df['Expected Cost'].idxmin()
opt_t   = thresh_df.loc[opt_idx, 'Threshold']
axes[1].axvline(opt_t, color='navy', linestyle=':', label=f'Grid optimum = {opt_t:.3f}')
axes[1].set_title('LDA: Expected Cost vs Threshold')
axes[1].set_xlabel('Threshold')
axes[1].legend(fontsize=8)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(f"Grid cost-optimal threshold : {opt_t:.3f}")
print(f"Theoretical threshold       : {p_star:.3f}")
```

### Performance at the cost-optimal threshold

```{python}
y_pred_lda_tuned = (y_prob_lda >= opt_t).astype(int)

print(f"=== LDA at threshold = {opt_t:.3f} ===")
print(classification_report(y_test, y_pred_lda_tuned,
                             target_names=['No Default', 'Default']))
print(f"Expected cost : {expected_cost(y_test, y_pred_lda_tuned, C):.4f}")
print(f"Expected cost at default threshold (0.50): "
      f"{expected_cost(y_test, y_pred_lda, C):.4f}")

fig, axes = plt.subplots(1, 2, figsize=(12, 4))
ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_lda),
                       display_labels=['No Default', 'Default']).plot(
    ax=axes[0], colorbar=False)
axes[0].set_title('LDA: Default threshold (0.50)')

ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_lda_tuned),
                       display_labels=['No Default', 'Default']).plot(
    ax=axes[1], colorbar=False)
axes[1].set_title(f'LDA: Cost-optimal threshold ({opt_t:.3f})')
plt.tight_layout()
plt.show()
```

**Interpretation:** Lowering the threshold from 0.50 to the grid-optimal **0.200** (close to the theoretical $p^* = 0.167$) raises default recall from **27 % to 63 %** and F1 from 0.38 to **0.50**, at the cost of dropping overall accuracy from 81 % to 72 % and precision on defaults from 0.65 to 0.42. The expected cost falls from **0.840 to 0.599** because each missed defaulter carries a 5× penalty. The grid optimum sits slightly above the theoretical threshold, reflecting the fact that the LDA probabilities are not perfectly calibrated; in practice the two values are close enough that the theoretical formula provides a reliable starting point.

---

## 7.4 Solution 2: Adjusted Class Prior (LDA)

A cleaner, model-level intervention: refit LDA with balanced priors $\pi_0 = \pi_1 = 0.5$ instead of the empirical 78 / 22 split. This shifts the decision boundary toward the majority class, raising default recall without touching the threshold.

```{python}
# Empirical (default) priors
lda_emp = LinearDiscriminantAnalysis(priors=None)
lda_emp.fit(X_train_scaled, y_train)

# Balanced priors
lda_bal = LinearDiscriminantAnalysis(priors=[0.5, 0.5])
lda_bal.fit(X_train_scaled, y_train)

# Inverse-frequency priors
n0 = (y_train == 0).sum()
n1 = (y_train == 1).sum()
pi0 = n1 / (n0 + n1)   # give majority class the minority weight
pi1 = n0 / (n0 + n1)
lda_inv = LinearDiscriminantAnalysis(priors=[pi0, pi1])
lda_inv.fit(X_train_scaled, y_train)

prior_rows = []
for name, model in [('LDA (empirical priors)', lda_emp),
                     ('LDA (balanced priors)',  lda_bal),
                     ('LDA (inverse-freq priors)', lda_inv)]:
    yp = model.predict(X_test_scaled)
    prior_rows.append({
        'Model':              name,
        'Accuracy':           accuracy_score(y_test, yp),
        'Precision':          precision_score(y_test, yp),
        'Recall (Default)':   recall_score(y_test, yp),
        'F1 (Default)':       f1_score(y_test, yp),
        'Expected Cost':      expected_cost(y_test, yp, C),
    })

prior_df = pd.DataFrame(prior_rows).set_index('Model').round(4)
display(prior_df)
```

**Interpretation:** Balanced priors (0.5/0.5) shift the LDA boundary so that more borderline clients are flagged as defaulters: recall jumps from 27 % to **60 %** and expected cost drops from 0.840 to **0.601**. Inverse-frequency priors (which assign the minority-class weight to the majority class) push the boundary even further, achieving 95 % recall, but at the cost of catastrophically low precision (0.24) and very poor overall accuracy (33 %), resulting in a worse expected cost of 0.713. For this dataset, **balanced priors offer the best trade-off among prior-adjustment strategies**, achieving a higher F1 (0.517 vs 0.386 for inverse-frequency) and lower expected cost.

---

## 7.5 Solution 3: Resampling

Data-level strategies modify the training set rather than the model. We compare three approaches and evaluate each at the default 0.5 threshold and at the cost-optimal threshold.

### Strategies

```{python}
from sklearn.utils import resample

# --- Random undersampling ---
X_tr_np = X_train_scaled.values
y_tr_np  = y_train.values

mask_maj = y_tr_np == 0
mask_min = y_tr_np == 1
n_min = mask_min.sum()

X_maj_down, y_maj_down = resample(
    X_tr_np[mask_maj], y_tr_np[mask_maj],
    replace=False, n_samples=n_min, random_state=42
)
X_down = np.vstack([X_maj_down, X_tr_np[mask_min]])
y_down = np.concatenate([y_maj_down, y_tr_np[mask_min]])

# --- Random oversampling ---
X_min_up, y_min_up = resample(
    X_tr_np[mask_min], y_tr_np[mask_min],
    replace=True, n_samples=mask_maj.sum(), random_state=42
)
X_up = np.vstack([X_tr_np[mask_maj], X_min_up])
y_up = np.concatenate([y_tr_np[mask_maj], y_min_up])

print(f"Original training set     : {y_tr_np.shape[0]} rows  "
      f"(default rate = {y_tr_np.mean():.3f})")
print(f"Undersampled training set : {y_down.shape[0]} rows  "
      f"(default rate = {y_down.mean():.3f})")
print(f"Oversampled training set  : {y_up.shape[0]} rows   "
      f"(default rate = {y_up.mean():.3f})")
```

```{python}
# Fit LDA on each training set
lda_orig = LinearDiscriminantAnalysis()
lda_orig.fit(X_tr_np, y_tr_np)

lda_down = LinearDiscriminantAnalysis()
lda_down.fit(X_down, y_down)

lda_up = LinearDiscriminantAnalysis()
lda_up.fit(X_up, y_up)

X_te_np = X_test_scaled.values
y_te_np = y_test.values

resample_rows = []
for name, model in [('Original (no resampling)', lda_orig),
                     ('Random undersampling',     lda_down),
                     ('Random oversampling',      lda_up)]:
    prob = model.predict_proba(X_te_np)[:, 1]
    yp_50  = (prob >= 0.50).astype(int)
    yp_opt = (prob >= opt_t).astype(int)
    resample_rows.append({
        'Strategy':                  name,
        'Recall@0.5':                recall_score(y_te_np, yp_50),
        'F1@0.5':                    f1_score(y_te_np, yp_50),
        'Expected Cost@0.5':         expected_cost(y_te_np, yp_50, C),
        f'Recall@{opt_t:.2f}':       recall_score(y_te_np, yp_opt),
        f'F1@{opt_t:.2f}':           f1_score(y_te_np, yp_opt),
        f'Expected Cost@{opt_t:.2f}':expected_cost(y_te_np, yp_opt, C),
        'ROC-AUC':                   roc_auc_score(y_te_np, prob),
    })

resample_df = pd.DataFrame(resample_rows).set_index('Strategy').round(4)
display(resample_df)
```

### Confusion matrices at threshold 0.5

```{python}
fig, axes = plt.subplots(1, 3, figsize=(15, 4))
labels = ['No Default', 'Default']

for ax, (name, model) in zip(axes, [
        ('Original',     lda_orig),
        ('Undersampled', lda_down),
        ('Oversampled',  lda_up)]):
    yp = model.predict(X_te_np)
    ConfusionMatrixDisplay(confusion_matrix(y_te_np, yp),
                           display_labels=labels).plot(ax=ax, colorbar=False)
    ax.set_title(f'LDA: {name}')

plt.tight_layout()
plt.show()
```

**Interpretation:**

* **Undersampling** reduces the training set from 21 000 to **9 290 rows** (50/50 balance by discarding majority-class observations). At threshold 0.5 it more than doubles default recall (27 % → **61 %**) and cuts expected cost from 0.840 to **0.613**. However, at the cost-optimal threshold 0.20 it pushes recall to 98 % with a precision collapse to 0.19, raising expected cost back to 0.732.
* **Oversampling** repeats minority-class rows to reach **32 710 training rows** (50/50). Results at threshold 0.5 are nearly identical to undersampling (recall 61 %, expected cost 0.610), confirming both strategies impose a similar effective prior shift. Oversampling preserves all original majority-class information, giving it a marginally higher AUC (0.741 vs 0.742) but no practical difference at this scale.
* Both resampling strategies improve expected cost at threshold 0.5 over the imbalanced baseline. When combined with the cost-optimal threshold (0.20), however, neither beats plain LDA with threshold tuning or balanced priors, so the extreme recall gain (∼98 %) destroys precision and inflates cost.

---

## 7.6 Overall Comparison

```{python}
all_rows = []

configs = [
    ('LDA: default threshold (0.50)',       y_pred_lda,        y_prob_lda),
    (f'LDA: cost threshold ({opt_t:.2f})',  y_pred_lda_tuned,  y_prob_lda),
    ('LDA: balanced priors (0.5/0.5)',      lda_bal.predict(X_test_scaled), 
                                              lda_bal.predict_proba(X_test_scaled)[:, 1]),
    ('LDA: oversample train',               (lda_up.predict_proba(X_te_np)[:, 1] >= 0.5).astype(int),
                                              lda_up.predict_proba(X_te_np)[:, 1]),
    ('GNB: default threshold (0.50)',       y_pred_gnb,        y_prob_gnb),
    (f'GNB: cost threshold ({opt_t:.2f})',  (y_prob_gnb >= opt_t).astype(int), y_prob_gnb),
]

for name, yp, yprob in configs:
    all_rows.append({
        'Configuration':      name,
        'Accuracy':           accuracy_score(y_test, yp),
        'Recall (Default)':   recall_score(y_test, yp),
        'Precision (Default)':precision_score(y_test, yp, zero_division=0),
        'F1 (Default)':       f1_score(y_test, yp),
        'ROC-AUC':            roc_auc_score(y_test, yprob),
        'Expected Cost':      expected_cost(y_test, yp, C),
    })

final_df = pd.DataFrame(all_rows).set_index('Configuration').round(4)
display(final_df)
```

### Key takeaways

* **Threshold tuning is the highest-leverage, zero-retraining intervention**: shifting LDA's threshold from 0.50 to **0.200** (grid-optimal; theoretical $p^* = 0.167$) raises default recall from 27 % to **63 %**, F1 from 0.38 to **0.50**, and delivers the **lowest expected cost of 0.599** across all configurations tested.
* **Balanced priors** are the best single model-level fix: one parameter change raises recall to **60 %** and F1 to **0.517** (the highest of any configuration) with expected cost of 0.601, which is essentially tied with threshold tuning, and more principled theoretically.
* **Resampling at threshold 0.5** brings comparable gains to balanced priors (recall ~61 %, expected cost ~0.61), but combining resampling with the cost-optimal threshold backfires: the model overshoots to ∼98 % recall with precision of 0.19, worsening expected cost to 0.73.
* **GNB with cost threshold (0.20)** achieves a recall of **59 %** and expected cost of **0.637**, but consistently behind the best LDA variants. GNB's miscalibrated probabilities make threshold selection less precise.
* **ROC-AUC is invariant across all interventions** (all LDA variants: 0.740–0.741): every strategy here improves decision rules without changing the underlying ranking ability of the model.
* The expected-cost framework makes the business trade-off explicit and portable. With a 5:1 FN/FP cost ratio the optimal threshold is 0.167; a bank with a different loss estimate simply updates $c_{FN}$ and recomputes $p^* = c_{FP}/(c_{FP}+c_{FN})$.