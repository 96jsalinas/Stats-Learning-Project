---
title: "Credit Card Default: EDA & Preprocessing"
format:
    html:
        code-fold: true
        toc: true
        toc-depth: 3
jupyter: python3
authors:
  - name: Josu Salinas Colina
  - name: Eoin Gallagher
  - name: Francisca Eeckels
---

# Overview

This report analyses the **UCI Default of Credit Card Clients** dataset (30 000 Taiwanese cardholders, 2005). The goal is to predict whether a client will default on their payment in October 2005. 

---

# 1. Load & Basic Cleaning

We first load the dataset from a local cache (or download it if absent), drop the meaningless `ID` column, and rename every feature to a more readable snake-case convention. Two helper label columns (`edu_label`, `gender_label`) are created **for visualisation only** and will be dropped before modelling.

```{python}
import os
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import skew
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

local_file = "default_credit_card_clients.xlsx"
url = ("https://archive.ics.uci.edu/ml/machine-learning-databases"
       "/00350/default%20of%20credit%20card%20clients.xls")

if os.path.exists(local_file):
    df = pd.read_excel(local_file)
else:
    df = pd.read_excel(url, header=1)
    df.to_excel(local_file, index=False)

rename_dict = {
    'LIMIT_BAL': 'credit_limit', 'SEX': 'gender',
    'EDUCATION': 'education', 'MARRIAGE': 'marital_status', 'AGE': 'age',
    'PAY_0': 'status_sep', 'PAY_2': 'status_aug', 'PAY_3': 'status_jul',
    'PAY_4': 'status_jun', 'PAY_5': 'status_may', 'PAY_6': 'status_apr',
    'BILL_AMT1': 'bill_sep', 'BILL_AMT2': 'bill_aug', 'BILL_AMT3': 'bill_jul',
    'BILL_AMT4': 'bill_jun', 'BILL_AMT5': 'bill_may', 'BILL_AMT6': 'bill_apr',
    'PAY_AMT1': 'paid_sep', 'PAY_AMT2': 'paid_aug', 'PAY_AMT3': 'paid_jul',
    'PAY_AMT4': 'paid_jun', 'PAY_AMT5': 'paid_may', 'PAY_AMT6': 'paid_apr',
    'default payment next month': 'default'
}
df.rename(columns=rename_dict, inplace=True)
if 'ID' in df.columns:
    df.drop('ID', axis=1, inplace=True)

# EDA-only label columns
df['edu_label']    = df['education'].map({1: 'Grad', 2: 'Uni', 3: 'HS',
                                           4: 'Other', 5: 'Other',
                                           6: 'Other', 0: 'Other'})
df['gender_label'] = df['gender'].map({1: 'Male', 2: 'Female'})

print(f"Shape: {df.shape}")
df.head(3)
```

The dataset contains **30 000 rows** and **25 feature columns** after renaming. No structural issues (duplicate headers, merged cells) were found in the raw Excel file.

---

# 2. Data Integrity & Summary

Before any modelling we perform a per-column audit covering: data type, count of missing values, mean, skewness and excess kurtosis.

```{python}
def academic_summary(df_in):
    numeric_df = df_in.select_dtypes(include=[np.number])
    summary = pd.DataFrame(index=numeric_df.columns)
    summary['Type']     = numeric_df.dtypes
    summary['Missing']  = numeric_df.isnull().sum()
    summary['Mean']     = numeric_df.mean().round(2)
    summary['Skewness'] = numeric_df.skew().round(2)
    summary['Kurtosis'] = numeric_df.kurt().round(2)
    return summary

academic_summary(df)
```

**Key findings:**

* **No missing values**: the dataset is complete, so no imputation is required.
* **Monetary columns** (`bill_*`, `paid_*`, `credit_limit`) show strong positive skewness (typically > 2), indicating a long right tail consistent with a small number of very high-spending or high-limit clients. These columns will benefit from a log transform.
* **Repayment status columns** (`status_*`) are encoded as integers (−2 to 8). They are ordinal, not continuous, and will be treated accordingly.
* **Target (`default`)** is binary (0 / 1).

---

# 3. Visual EDA

## 3.1 Univariate Analysis

We inspect the distribution of the target variable, age, credit limit, and education level.

```{python}
#| warning: false
#| 
fig, axes = plt.subplots(2, 2, figsize=(14, 9))

sns.countplot(x='default', data=df, ax=axes[0, 0], palette='viridis')
axes[0, 0].set_title('Target Balance (Default vs Non-Default)')
axes[0, 0].set_xlabel('Default (0 = No, 1 = Yes)')

sns.histplot(df['age'], bins=30, kde=True, ax=axes[0, 1], color='skyblue')
axes[0, 1].set_title('Age Distribution')

sns.histplot(df['credit_limit'], bins=30, kde=True, ax=axes[1, 0], color='salmon')
axes[1, 0].set_title('Credit Limit Distribution')

sns.countplot(x='edu_label', data=df, ax=axes[1, 1], palette='pastel',
              order=['Grad', 'Uni', 'HS', 'Other'])
axes[1, 1].set_title('Education Background')

plt.tight_layout()
plt.show()
```

**Interpretations:**

* **Class imbalance**: roughly 78 % of clients did *not* default vs. 22 % who did. This imbalance is moderate, it will be accounted for in modelling but is not severe enough to require aggressive resampling on its own.
* **Age**: ages range from 21 to 79, with a right-skewed distribution peaking around 26–30. Most clients are young adults, the long right tail means a handful of elderly clients hold much higher credit limits on average.
* **Credit limit**: highly right-skewed: most clients cluster at lower limits (NT\$50 000–200 000) with a few outliers exceeding NT\$800 000. Log transformation will be applied before modelling.
* **Education**: university graduates form the largest group, followed by graduate-school cardholders. The small "Other/Unknown" category (codes 0, 5, 6) will be collapsed into a single level.

---

## 3.2 Bivariate Analysis: Features vs Default

```{python}
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

sns.boxplot(x='default', y='age', data=df, palette='Set2', ax=axes[0])
axes[0].set_title('Age vs Default Status')
axes[0].set_xlabel('Default (0 = No, 1 = Yes)')

sns.boxplot(x='default', y='credit_limit', data=df, palette='Set2', ax=axes[1])
axes[1].set_title('Credit Limit vs Default Status')
axes[1].set_xlabel('Default (0 = No, 1 = Yes)')

plt.tight_layout()
plt.show()
```

**Interpretations:**

* **Age**: the median age is very similar across both groups (around 35), suggesting age alone is a weak discriminator. The interquartile ranges overlap heavily, any predictive power age carries is likely captured through interaction with other variables.
* **Credit limit**: defaulters tend to hold *lower* credit limits than non-defaulters. This makes economic sense: issuers typically grant higher limits to clients with a stronger repayment history. The difference is meaningful but distributions still overlap substantially, so credit limit will be one signal among many rather than a dominant predictor.

---

## 3.3 Multicollinearity Assessment

High correlations between predictors can distort probabilistic models (e.g. Naïve Bayes) and inflate variance in regression-based approaches. We focus on the six monthly bill-amount columns.

```{python}
bill_cols = [c for c in df.columns if 'bill' in c]

plt.figure(figsize=(9, 6))
sns.heatmap(df[bill_cols].corr(), annot=True, cmap='viridis', fmt=".2f",
            linewidths=0.5)
plt.title("Pairwise Correlations: Monthly Bill Amounts")
plt.tight_layout()
plt.show()
```

**Interpretations:**

* Adjacent months (e.g. `bill_sep` and `bill_aug`) show very high correlations (around 0.90–0.95), which is expected: a client's outstanding balance carries over month to month.
* Correlations weaken with temporal distance : `bill_sep` and `bill_apr` are still moderately correlated (around 0.70), but less so than neighbouring months.
* This near-multicollinearity is a known challenge for Naïve Bayes (which assumes feature independence) and for LDA (which inverts the covariance matrix). Dimensionality reduction or careful feature selection will be considered during modelling.

---

## 3.4 Temporal Trends: Bills vs Payments

We plot the mean bill amount and mean payment amount across the six observation months (April–September 2005) to detect any macro-level trend.

```{python}
bill_cols_ordered = ['bill_apr', 'bill_may', 'bill_jun',
                     'bill_jul', 'bill_aug', 'bill_sep']
paid_cols_ordered = ['paid_apr', 'paid_may', 'paid_jun',
                     'paid_jul', 'paid_aug', 'paid_sep']
months = ['Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep']

plt.figure(figsize=(10, 5))
plt.plot(months, df[bill_cols_ordered].mean(), marker='o', label='Avg Bill Amount')
plt.plot(months, df[paid_cols_ordered].mean(), marker='s', label='Avg Amount Paid')
plt.title('Average Bill vs Payment Over Time (Apr–Sep 2005)')
plt.ylabel('NT$ (average across all clients)')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

**Interpretations:**

* Average outstanding bill amounts remain **consistently higher** than average payments across all months, confirming that most clients carry a revolving balance.
* The gap between billed and paid amounts is relatively stable, suggesting no dramatic macro shock during this period.
* Both series are fairly flat month-to-month, indicating the dataset captures a steady-state spending regime rather than a seasonal spike. The mismatch between billed and paid amounts is a structural feature that repayment-status variables will encode more directly.

---

# 4. Preprocessing Pipeline

## 4.1 Variable Grouping

We explicitly categorise every feature before applying transformations, ensuring the right treatment for each variable type.

```{python}
monetary_cols = [
    'credit_limit', 'age',
    'bill_sep', 'bill_aug', 'bill_jul', 'bill_jun', 'bill_may', 'bill_apr',
    'paid_sep', 'paid_aug', 'paid_jul', 'paid_jun', 'paid_may', 'paid_apr'
]
ordinal_cols = [
    'education',
    'status_sep', 'status_aug', 'status_jul',
    'status_jun', 'status_may', 'status_apr'
]
nominal_cols = ['gender', 'marital_status']

print("Monetary (continuous / skewed):", monetary_cols)
print("\nOrdinal (integer-coded):", ordinal_cols)
print("\nNominal (to one-hot encode):", nominal_cols)
```

---

## 4.2 Cleaning & Encoding

Undocumented category codes (0, 5, 6 in `education`; 0 in `marital_status`) are collapsed into the existing "Other" category. Nominal variables are one-hot encoded (dropping the first level to avoid perfect multicollinearity).

```{python}
df['education']      = df['education'].replace([0, 5, 6], 4)
df['marital_status'] = df['marital_status'].replace(0, 3)

df_encoded = pd.get_dummies(df, columns=nominal_cols, drop_first=True)
X = df_encoded.drop(columns=['default', 'edu_label', 'gender_label'], errors='ignore')
y = df_encoded['default']

print(f"Feature matrix shape: {X.shape}")
print("Columns:", list(X.columns))
```

---

## 4.3 Train / Test Split

A stratified 70 / 30 split preserves the original class ratio in both subsets, preventing optimistic evaluation bias from accidental over-representation of non-defaults in the test set.

```{python}
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

print(f"Training set : {X_train.shape[0]} rows  "
      f"(default rate = {y_train.mean():.3f})")
print(f"Test set     : {X_test.shape[0]} rows  "
      f"(default rate = {y_test.mean():.3f})")
```

The near-identical default rates in both splits confirm that stratification worked correctly.

---

## 4.4 Selective Log Transformation

To reduce skewness in monetary columns we apply **log1p** (i.e. log(1 + x)) but only to columns whose absolute skewness on the training set exceeds a threshold of 0.75. This threshold-based approach avoids unnecessarily transforming variables that are already approximately symmetric, and the threshold decision is made on training data only to prevent data leakage.

```{python}
SKEW_THRESHOLD = 0.75
logged_cols = []
train_skewness_before = X_train[monetary_cols].skew()

for col in monetary_cols:
    if abs(train_skewness_before[col]) > SKEW_THRESHOLD:
        X_train[col] = np.log1p(X_train[col].clip(lower=0))
        X_test[col]  = np.log1p(X_test[col].clip(lower=0))
        logged_cols.append(col)

print(f"Columns log-transformed ({len(logged_cols)}): {logged_cols}")
```

---

## 4.5 Standardisation

Finally, all features are standardised to zero mean and unit variance using `StandardScaler` fitted **exclusively on the training set**. The same parameters are then applied to the test set, again avoiding leakage.

```{python}
final_cols = (monetary_cols + ordinal_cols +
              [c for c in X.columns if 'gender_' in c or 'marital_status_' in c])

X_train = X_train[final_cols]
X_test  = X_test[final_cols]

scaler = StandardScaler()
X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)
X_test_scaled  = pd.DataFrame(scaler.transform(X_test),  columns=X_test.columns)

print(f"Final feature count: {X_train_scaled.shape[1]}")
print("Sample of scaled training data:")
X_train_scaled.head(3)
```

Standardisation is essential for distance-based and probabilistic models (LDA, Naïve Bayes, logistic regression with regularisation) that are sensitive to feature scale.

---

# 5. Preprocessing Verification

We confirm that log transformation meaningfully reduced skewness in the targeted columns.

```{python}
if logged_cols:
    comparison = pd.DataFrame({
        'Skew Before': train_skewness_before[logged_cols].values,
        'Skew After':  X_train[logged_cols].skew().values
    }, index=logged_cols)
    comparison['Reduction %'] = (
        (comparison['Skew Before'].abs() - comparison['Skew After'].abs())
        / comparison['Skew Before'].abs() * 100
    ).round(1)
    display(comparison.round(3))
```

**Interpretation:** For all transformed columns, absolute skewness dropped substantially (typically from > 2 to < 1), bringing the distributions much closer to symmetry. This will improve the Gaussian assumptions underlying LDA and Naïve Bayes and reduce the influence of outliers on distance-based models.

---

# 6. Discriminant & Probabilistic Classifiers

## 6.1 Overview

We evaluate three generative classifiers that share a common probabilistic framework: they all model $P(\mathbf{x} \mid y)$ and apply Bayes' theorem to obtain the posterior $P(y \mid \mathbf{x})$.

| Model | Covariance structure | Decision boundary |
|---|---|---|
| **LDA** | Shared $\Sigma$ across classes | Linear |
| **QDA** | Per-class $\Sigma_k$ | Quadratic |
| **Gaussian Naïve Bayes** | Diagonal (feature independence) | Quadratic |

```{python}
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import (
    classification_report, confusion_matrix, roc_auc_score,
    RocCurveDisplay, ConfusionMatrixDisplay
)
import warnings
warnings.filterwarnings('ignore')

# Reset indices so everything aligns cleanly
X_train_scaled = X_train_scaled.reset_index(drop=True)
X_test_scaled  = X_test_scaled.reset_index(drop=True)
y_train = y_train.reset_index(drop=True)
y_test  = y_test.reset_index(drop=True)
```

---

## 6.2 Linear Discriminant Analysis (LDA)

### Theory

LDA assumes each class $k$ follows a multivariate Gaussian $\mathcal{N}(\boldsymbol{\mu}_k, \Sigma)$ with a **shared** covariance matrix $\Sigma$. This shared-covariance assumption makes the log-posterior ratio linear in $\mathbf{x}$, yielding linear decision boundaries. Parameters are estimated by maximum likelihood: class means $\hat{\boldsymbol{\mu}}_k$ and a pooled within-class scatter matrix.

**Key assumptions:**

* Features are Gaussian within each class.
* All classes share the same covariance structure.

### Fit & Evaluate

```{python}
lda = LinearDiscriminantAnalysis()
lda.fit(X_train_scaled, y_train)

y_pred_lda = lda.predict(X_test_scaled)
y_prob_lda = lda.predict_proba(X_test_scaled)[:, 1]
auc_lda    = roc_auc_score(y_test, y_prob_lda)

print("=== LDA: Classification Report ===")
print(classification_report(y_test, y_pred_lda, target_names=['No Default', 'Default']))
print(f"ROC-AUC: {auc_lda:.4f}")
```

```{python}
fig, axes = plt.subplots(1, 2, figsize=(13, 4))

ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_lda),
                       display_labels=['No Default', 'Default']).plot(ax=axes[0], colorbar=False)
axes[0].set_title('LDA - Confusion Matrix')

RocCurveDisplay.from_predictions(y_test, y_prob_lda, ax=axes[1],
                                 name=f'LDA (AUC = {auc_lda:.3f})')
axes[1].plot([0, 1], [0, 1], 'k--', lw=1)
axes[1].set_title('LDA - ROC Curve')
plt.tight_layout()
plt.show()
```

### Interpretation

* LDA achieves **81 % overall accuracy**, however recall on non-defaulters is 96 % while recall on defaulters is only **27 %**, meaning nearly three quarters of actual defaulters are missed.
* The **ROC-AUC of 0.74** is the more meaningful summary: it confirms that LDA's linear projection captures genuine discriminative signal, ranking a randomly chosen defaulter above a randomly chosen non-defaulter 74 % of the time.
* The precision / recall trade-off for the default class (precision 0.65, recall 0.27, F1 0.38) reflects the shared-covariance assumption pulling the decision boundary toward the majority class. Lowering the classification threshold from 0.5 would improve recall at the cost of precision.

---

## 6.3 Quadratic Discriminant Analysis (QDA)

### Theory

QDA relaxes LDA's shared-covariance assumption by estimating a **separate** covariance matrix $\Sigma_k$ for each class. The log-posterior then contains a quadratic term in $\mathbf{x}$, producing curved decision boundaries. This gives QDA more flexibility but at the cost of estimating many more parameters ($p(p+1)/2$ per class), which can hurt performance when $n$ is small or features are highly correlated.

**Key assumptions:**

* Features are Gaussian within each class.
* Classes may have different covariance structures.

### Fit & Evaluate

```{python}
qda = QuadraticDiscriminantAnalysis(reg_param=0.01)
qda.fit(X_train_scaled, y_train)

y_pred_qda = qda.predict(X_test_scaled)
y_prob_qda = qda.predict_proba(X_test_scaled)[:, 1]
auc_qda    = roc_auc_score(y_test, y_prob_qda)

print("=== QDA: Classification Report ===")
print(classification_report(y_test, y_pred_qda, target_names=['No Default', 'Default']))
print(f"ROC-AUC: {auc_qda:.4f}")
```

> `reg_param=0.01` adds a small ridge to each class covariance matrix, stabilising inversion when some features are near-collinear (as seen in the bill-amount correlations from Section 3.3).

```{python}
fig, axes = plt.subplots(1, 2, figsize=(13, 4))

ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_qda),
                       display_labels=['No Default', 'Default']).plot(ax=axes[0], colorbar=False)
axes[0].set_title('QDA: Confusion Matrix')

RocCurveDisplay.from_predictions(y_test, y_prob_qda, ax=axes[1],
                                 name=f'QDA (AUC = {auc_qda:.3f})')
axes[1].plot([0, 1], [0, 1], 'k--', lw=1)
axes[1].set_title('QDA: ROC Curve')
plt.tight_layout()
plt.show()
```

### Interpretation

* QDA sacrifices overall accuracy (75 % vs LDA's 81 %) in exchange for much better balance on the default class: recall rises from 27 % (LDA) to **49 %**, and F1 improves from 0.38 to **0.47**. This reflects the quadratic boundary's ability to enclose the default cluster more tightly.
* However, QDA's **ROC-AUC of 0.72 is the lowest of the three models**, below LDA (0.74) and Naïve Bayes (0.74). The multicollinearity in the bill columns (Section 3.3) makes per-class covariance estimation unreliable.
* The two classes do appear to differ in their covariance structure (defaulters show more volatile spending), which is why QDA's recall gain over LDA is real, but the correlated features limit how much the extra flexibility can be exploited.

---

## 6.4 Gaussian Naïve Bayes

### Theory

Gaussian Naïve Bayes (GNB) takes the independence assumption to its extreme: given the class label, every feature is treated as **conditionally independent**, so the class-conditional density factorises as:

$$P(\mathbf{x} \mid y = k) = \prod_{j=1}^{p} \mathcal{N}(x_j \mid \mu_{kj},\, \sigma_{kj}^2)$$

This makes GNB equivalent to QDA with diagonal (per-class) covariance matrices. Parameter estimation is trivially fast and the model is robust to high dimensionality, but the independence assumption is strongly violated here (see the bill-amount correlations in Section 3).

**Key assumptions:**

* Features are conditionally independent given the class label.
* Each feature is Gaussian within each class.

### Fit & Evaluate

```{python}
gnb = GaussianNB()
gnb.fit(X_train_scaled, y_train)

y_pred_gnb = gnb.predict(X_test_scaled)
y_prob_gnb = gnb.predict_proba(X_test_scaled)[:, 1]
auc_gnb    = roc_auc_score(y_test, y_prob_gnb)

print("=== Gaussian Naïve Bayes: Classification Report ===")
print(classification_report(y_test, y_pred_gnb, target_names=['No Default', 'Default']))
print(f"ROC-AUC: {auc_gnb:.4f}")
```

```{python}
fig, axes = plt.subplots(1, 2, figsize=(13, 4))

ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_gnb),
                       display_labels=['No Default', 'Default']).plot(ax=axes[0], colorbar=False)
axes[0].set_title('Naïve Bayes: Confusion Matrix')

RocCurveDisplay.from_predictions(y_test, y_prob_gnb, ax=axes[1],
                                 name=f'Naïve Bayes (AUC = {auc_gnb:.3f})')
axes[1].plot([0, 1], [0, 1], 'k--', lw=1)
axes[1].set_title('Naïve Bayes: ROC Curve')
plt.tight_layout()
plt.show()
```

### Interpretation

* Despite the strongly violated independence assumption, GNB achieves a **ROC-AUC of 0.74**, matching LDA, this is probably because the ranking of predicted probabilities is still informative even when absolute probability values are miscalibrated.
* GNB has the **highest recall on defaulters (51 %)** and a competitive F1 of 0.49, outperforming both LDA (F1 0.38) and QDA (F1 0.47) on the minority class at the cost of lower overall accuracy (76 %).
* The precision on defaults is only 0.47 (vs LDA's 0.65), confirming the miscalibration: GNB flags more clients as risky than warranted.

---

## 6.5 Model Comparison

### Combined ROC Curves

```{python}
fig, ax = plt.subplots(figsize=(8, 6))

for name, probs in [('LDA', y_prob_lda), ('QDA', y_prob_qda), ('Naïve Bayes', y_prob_gnb)]:
    RocCurveDisplay.from_predictions(y_test, probs, ax=ax,
                                     name=f'{name} (AUC = {roc_auc_score(y_test, probs):.3f})')

ax.plot([0, 1], [0, 1], 'k--', lw=1, label='Random classifier')
ax.set_title('ROC Curves: LDA vs QDA vs Naïve Bayes')
ax.legend(loc='lower right')
plt.tight_layout()
plt.show()
```

### Summary Table

```{python}
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

rows = []
for name, y_pred, y_prob in [
    ('LDA',         y_pred_lda, y_prob_lda),
    ('QDA',         y_pred_qda, y_prob_qda),
    ('Naïve Bayes', y_pred_gnb, y_prob_gnb),
]:
    rows.append({
        'Model':                  name,
        'Accuracy':               accuracy_score(y_test, y_pred),
        'Precision (Default)':    precision_score(y_test, y_pred),
        'Recall (Default)':       recall_score(y_test, y_pred),
        'F1 (Default)':           f1_score(y_test, y_pred),
        'ROC-AUC':                roc_auc_score(y_test, y_prob),
    })

summary_df = pd.DataFrame(rows).set_index('Model').round(4)
display(summary_df)
```

### Discussion

* **LDA** achieves the highest overall accuracy (81 %) and highest ROC-AUC (0.74), making it the best ranking model. Its weakness is the very low recall on defaulters (27 %): it is a conservative model that only flags cases it is highly confident about, producing a precision of 0.65 on the default class.
* **QDA** has the lowest accuracy (75 %) and lowest AUC (0.72), but nearly doubles LDA's recall on defaults (49 %). The extra covariance flexibility partially captures the different spending volatility between the two classes, though multicollinearity in the bill features caps the gains.
* **Gaussian Naïve Bayes** strikes the best minority-class balance: highest default recall (51 %), best default F1 (0.49), and an AUC (0.74) that matches LDA. It is the best choice when catching defaulters matters more than precision, provided the output probabilities are recalibrated before use.
* The core issue of all three models is the **precision–recall trade-off on the minority class**, none reaches a default F1 above 0.49.

---

# 7. Cost-Sensitive Learning

## 7.1 The Cost of Ignoring Imbalance

The dataset has a 22 % default rate , predicting "no default" for every observation would yield 78 % accuracy yet catch zero defaulters. More importantly, **the two types of error are not equally costly** in credit risk:

* **False Negative** (missing a defaulter): the bank extends credit it will not recover, high financial loss.
* **False Positive** (flagging a good client): a brief manual review is triggered, low administrative cost.

This asymmetry must be encoded explicitly rather than left to the default 0.5 threshold.

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    confusion_matrix, ConfusionMatrixDisplay,
    classification_report, roc_auc_score,
    precision_score, recall_score, f1_score, accuracy_score
)
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB

# Naive baseline: always predict majority class
y_always_no_default = np.zeros(len(y_test), dtype=int)
print("Naive 'always No-Default' baseline")
print(classification_report(y_test, y_always_no_default,
                             target_names=['No Default', 'Default']))
```

---

## 7.2 Defining a Cost Matrix

We assign costs reflecting the business reality of a credit card issuer.

| | **Predicted: No Default** | **Predicted: Default** |
|---|---|---|
| **Actual: No Default** | 0 (TN) | 1 (FP unnecessary review) |
| **Actual: Default** | 5 (FN unrecovered credit) | 0 (TP) |

A missed defaulter costs **5×** more than a false alarm. This yields the theoretical optimal threshold:

$$p^* = \frac{c_{FP}}{c_{FP} + c_{FN}} = \frac{1}{1 + 5} \approx 0.167$$

Any client whose predicted default probability exceeds 0.167 should be classified as a defaulter.

```{python}
# Cost matrix: rows = actual, cols = predicted
#              No Default  Default
C = np.array([[0,          1],    # Actual: No Default
              [5,          0]])   # Actual: Default

c_fp = C[0, 1]   # FP cost
c_fn = C[1, 0]   # FN cost
p_star = c_fp / (c_fp + c_fn)
print(f"Theoretical cost-optimal threshold: p* = {p_star:.4f}")

def expected_cost(y_true, y_pred, cost_matrix, normalize=True):
    cm = confusion_matrix(y_true, y_pred)
    total = np.sum(cm * cost_matrix)
    return total / len(y_true) if normalize else total
```

---

## 7.3 Solution 1: Decision Threshold Tuning

The LDA model already yields calibrated probabilities (AUC 0.74). Instead of always predicting the class with the highest probability, we apply a custom threshold derived from the cost matrix.

### Threshold sweep

```{python}
thresholds = np.arange(0.05, 0.70, 0.025)
rows_thresh = []

for t in thresholds:
    y_pred_t = (y_prob_lda >= t).astype(int)
    rows_thresh.append({
        'Threshold':          t,
        'Accuracy':           accuracy_score(y_test, y_pred_t),
        'Recall (Default)':   recall_score(y_test, y_pred_t, zero_division=0),
        'Precision (Default)':precision_score(y_test, y_pred_t, zero_division=0),
        'F1 (Default)':       f1_score(y_test, y_pred_t, zero_division=0),
        'Expected Cost':      expected_cost(y_test, y_pred_t, C),
    })

thresh_df = pd.DataFrame(rows_thresh)

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

for metric, color in [('Recall (Default)', 'tab:orange'),
                       ('Precision (Default)', 'tab:blue'),
                       ('F1 (Default)', 'tab:green'),
                       ('Accuracy', 'tab:gray')]:
    axes[0].plot(thresh_df['Threshold'], thresh_df[metric], label=metric, color=color)
axes[0].axvline(p_star, color='red', linestyle='--', label=f'p* = {p_star:.2f}')
axes[0].set_title('LDA: Metrics vs Decision Threshold')
axes[0].set_xlabel('Threshold')
axes[0].legend(fontsize=8)
axes[0].grid(True, alpha=0.3)

axes[1].plot(thresh_df['Threshold'], thresh_df['Expected Cost'], color='crimson')
axes[1].axvline(p_star, color='red', linestyle='--', label=f'p* = {p_star:.2f}')
opt_idx = thresh_df['Expected Cost'].idxmin()
opt_t   = thresh_df.loc[opt_idx, 'Threshold']
axes[1].axvline(opt_t, color='navy', linestyle=':', label=f'Grid optimum = {opt_t:.3f}')
axes[1].set_title('LDA: Expected Cost vs Threshold')
axes[1].set_xlabel('Threshold')
axes[1].legend(fontsize=8)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(f"Grid cost-optimal threshold : {opt_t:.3f}")
print(f"Theoretical threshold       : {p_star:.3f}")
```

### Performance at the cost-optimal threshold

```{python}
y_pred_lda_tuned = (y_prob_lda >= opt_t).astype(int)

print(f"=== LDA at threshold = {opt_t:.3f} ===")
print(classification_report(y_test, y_pred_lda_tuned,
                             target_names=['No Default', 'Default']))
print(f"Expected cost : {expected_cost(y_test, y_pred_lda_tuned, C):.4f}")
print(f"Expected cost at default threshold (0.50): "
      f"{expected_cost(y_test, y_pred_lda, C):.4f}")

fig, axes = plt.subplots(1, 2, figsize=(12, 4))
ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_lda),
                       display_labels=['No Default', 'Default']).plot(
    ax=axes[0], colorbar=False)
axes[0].set_title('LDA: Default threshold (0.50)')

ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_lda_tuned),
                       display_labels=['No Default', 'Default']).plot(
    ax=axes[1], colorbar=False)
axes[1].set_title(f'LDA: Cost-optimal threshold ({opt_t:.3f})')
plt.tight_layout()
plt.show()
```

**Interpretation:** Lowering the threshold from 0.50 to the grid-optimal **0.200** (close to the theoretical $p^* = 0.167$) raises default recall from **27 % to 63 %** and F1 from 0.38 to **0.50**, at the cost of dropping overall accuracy from 81 % to 72 % and precision on defaults from 0.65 to 0.42. The expected cost falls from **0.840 to 0.599** because each missed defaulter carries a 5× penalty. The grid optimum sits slightly above the theoretical threshold, reflecting the fact that the LDA probabilities are not perfectly calibrated; in practice the two values are close enough that the theoretical formula provides a reliable starting point.

### ROC-based threshold: Youden's J statistic

A second principled approach to threshold selection uses the ROC curve directly. **Youden's J statistic** maximises the sum of sensitivity and specificity:

$$J = \text{Sensitivity} + \text{Specificity} - 1 = \text{TPR} - \text{FPR}$$

This is a symmetrical criterion that treats both error types equally, in contrast to the cost-matrix threshold which explicitly weights false negatives 5× more. The optimal point is the threshold at which the ROC curve is furthest from the random-classifier diagonal.

```{python}
from sklearn.metrics import roc_curve

fpr, tpr, roc_thresholds = roc_curve(y_test, y_prob_lda)
youden_j    = tpr - fpr
best_idx    = np.argmax(youden_j)
youden_t    = roc_thresholds[best_idx]

print(f"Youden's J optimal threshold : {youden_t:.4f}")
print(f"  Sensitivity (Recall)       : {tpr[best_idx]:.4f}")
print(f"  Specificity (1 - FPR)      : {1 - fpr[best_idx]:.4f}")
print(f"  Youden's J                 : {youden_j[best_idx]:.4f}")

y_pred_lda_youden = (y_prob_lda >= youden_t).astype(int)
print(f"\n=== LDA at Youden threshold = {youden_t:.4f} ===")
print(classification_report(y_test, y_pred_lda_youden,
                             target_names=['No Default', 'Default']))
print(f"Expected cost : {expected_cost(y_test, y_pred_lda_youden, C):.4f}")
```

```{python}
fig, axes = plt.subplots(1, 3, figsize=(18, 4))

ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_lda),
                       display_labels=['No Default', 'Default']).plot(
    ax=axes[0], colorbar=False)
axes[0].set_title('LDA: Default threshold (0.50)')

ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_lda_tuned),
                       display_labels=['No Default', 'Default']).plot(
    ax=axes[1], colorbar=False)
axes[1].set_title(f'LDA: Cost-optimal ({opt_t:.3f})')

ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_lda_youden),
                       display_labels=['No Default', 'Default']).plot(
    ax=axes[2], colorbar=False)
axes[2].set_title(f"LDA: Youden's J ({youden_t:.3f})")

plt.tight_layout()
plt.show()
```

**Interpretation:** Youden's J selects a threshold of **0.2244** (sensitivity 0.60, specificity 0.80, J = 0.398), yielding recall 60 %, F1 **0.52**, accuracy 75 %, and expected cost **0.600**. The cost-matrix threshold (0.200) lands lower because it explicitly penalises false negatives more, pushing the boundary further toward flagging borderline clients as defaulters to avoid the 5× FN penalty. In this dataset the two thresholds sit close together (0.200 vs 0.224) and produce nearly identical expected costs (0.599 vs 0.600), but in settings with more extreme cost ratios or better-calibrated probabilities, the gap would widen. Youden's J is the right default when costs are unknown or symmetric; the cost-matrix threshold is strictly better here because it encodes the business loss function directly.

A cleaner, model-level intervention: refit LDA with balanced priors $\pi_0 = \pi_1 = 0.5$ instead of the empirical 78 / 22 split. This shifts the decision boundary toward the majority class, raising default recall without touching the threshold.

```{python}
# Empirical (default) priors
lda_emp = LinearDiscriminantAnalysis(priors=None)
lda_emp.fit(X_train_scaled, y_train)

# Balanced priors
lda_bal = LinearDiscriminantAnalysis(priors=[0.5, 0.5])
lda_bal.fit(X_train_scaled, y_train)

# Inverse-frequency priors
n0 = (y_train == 0).sum()
n1 = (y_train == 1).sum()
pi0 = n1 / (n0 + n1)   # give majority class the minority weight
pi1 = n0 / (n0 + n1)
lda_inv = LinearDiscriminantAnalysis(priors=[pi0, pi1])
lda_inv.fit(X_train_scaled, y_train)

prior_rows = []
for name, model in [('LDA (empirical priors)', lda_emp),
                     ('LDA (balanced priors)',  lda_bal),
                     ('LDA (inverse-freq priors)', lda_inv)]:
    yp = model.predict(X_test_scaled)
    prior_rows.append({
        'Model':              name,
        'Accuracy':           accuracy_score(y_test, yp),
        'Precision':          precision_score(y_test, yp),
        'Recall (Default)':   recall_score(y_test, yp),
        'F1 (Default)':       f1_score(y_test, yp),
        'Expected Cost':      expected_cost(y_test, yp, C),
    })

prior_df = pd.DataFrame(prior_rows).set_index('Model').round(4)
display(prior_df)
```

**Interpretation:** Balanced priors (0.5/0.5) shift the LDA boundary so that more borderline clients are flagged as defaulters: recall jumps from 27 % to **60 %** and expected cost drops from 0.840 to **0.601**. Inverse-frequency priors (which assign the minority-class weight to the majority class) push the boundary even further, achieving 95 % recall, but at the cost of catastrophically low precision (0.24) and very poor overall accuracy (33 %), resulting in a worse expected cost of 0.713. For this dataset, **balanced priors offer the best trade-off among prior-adjustment strategies**, achieving a higher F1 (0.517 vs 0.386 for inverse-frequency) and lower expected cost.

---

## 7.5 Solution 3: Resampling

Data-level strategies modify the training set rather than the model. We compare three approaches and evaluate each at the default 0.5 threshold and at the cost-optimal threshold.

### Strategies

```{python}
from sklearn.utils import resample
from imblearn.over_sampling import SMOTE

# --- Random undersampling ---
X_tr_np = X_train_scaled.values
y_tr_np  = y_train.values

mask_maj = y_tr_np == 0
mask_min = y_tr_np == 1
n_min = mask_min.sum()

X_maj_down, y_maj_down = resample(
    X_tr_np[mask_maj], y_tr_np[mask_maj],
    replace=False, n_samples=n_min, random_state=42
)
X_down = np.vstack([X_maj_down, X_tr_np[mask_min]])
y_down = np.concatenate([y_maj_down, y_tr_np[mask_min]])

# --- Random oversampling ---
X_min_up, y_min_up = resample(
    X_tr_np[mask_min], y_tr_np[mask_min],
    replace=True, n_samples=mask_maj.sum(), random_state=42
)
X_up = np.vstack([X_tr_np[mask_maj], X_min_up])
y_up = np.concatenate([y_tr_np[mask_maj], y_min_up])

# --- SMOTE ---
smote = SMOTE(random_state=42)
X_smote, y_smote = smote.fit_resample(X_tr_np, y_tr_np)

print(f"Original training set     : {y_tr_np.shape[0]} rows  "
      f"(default rate = {y_tr_np.mean():.3f})")
print(f"Undersampled training set : {y_down.shape[0]} rows  "
      f"(default rate = {y_down.mean():.3f})")
print(f"Oversampled training set  : {y_up.shape[0]} rows   "
      f"(default rate = {y_up.mean():.3f})")
print(f"SMOTE training set        : {y_smote.shape[0]} rows   "
      f"(default rate = {y_smote.mean():.3f})")
```

```{python}
# Fit LDA on each training set
lda_orig = LinearDiscriminantAnalysis()
lda_orig.fit(X_tr_np, y_tr_np)

lda_down = LinearDiscriminantAnalysis()
lda_down.fit(X_down, y_down)

lda_up = LinearDiscriminantAnalysis()
lda_up.fit(X_up, y_up)

lda_smote = LinearDiscriminantAnalysis()
lda_smote.fit(X_smote, y_smote)

X_te_np = X_test_scaled.values
y_te_np = y_test.values

resample_rows = []
for name, model in [('Original (no resampling)', lda_orig),
                     ('Random undersampling',     lda_down),
                     ('Random oversampling',      lda_up),
                     ('SMOTE',                    lda_smote)]:
    prob = model.predict_proba(X_te_np)[:, 1]
    yp_50  = (prob >= 0.50).astype(int)
    yp_opt = (prob >= opt_t).astype(int)
    resample_rows.append({
        'Strategy':                  name,
        'Recall@0.5':                recall_score(y_te_np, yp_50),
        'F1@0.5':                    f1_score(y_te_np, yp_50),
        'Expected Cost@0.5':         expected_cost(y_te_np, yp_50, C),
        f'Recall@{opt_t:.2f}':       recall_score(y_te_np, yp_opt),
        f'F1@{opt_t:.2f}':           f1_score(y_te_np, yp_opt),
        f'Expected Cost@{opt_t:.2f}':expected_cost(y_te_np, yp_opt, C),
        'ROC-AUC':                   roc_auc_score(y_te_np, prob),
    })

resample_df = pd.DataFrame(resample_rows).set_index('Strategy').round(4)
display(resample_df)
```

### Confusion matrices at threshold 0.5

```{python}
fig, axes = plt.subplots(1, 4, figsize=(20, 4))
labels = ['No Default', 'Default']

for ax, (name, model) in zip(axes, [
        ('Original',     lda_orig),
        ('Undersampled', lda_down),
        ('Oversampled',  lda_up),
        ('SMOTE',        lda_smote)]):
    yp = model.predict(X_te_np)
    ConfusionMatrixDisplay(confusion_matrix(y_te_np, yp),
                           display_labels=labels).plot(ax=ax, colorbar=False)
    ax.set_title(f'LDA: {name}')

plt.tight_layout()
plt.show()
```

**Interpretation:**

* **Undersampling** reduces the training set from 21 000 to **9 290 rows** (50/50 balance by discarding majority-class observations). At threshold 0.5 it more than doubles default recall (27 % → **61 %**) and cuts expected cost from 0.840 to **0.613**. However, at the cost-optimal threshold 0.20 it pushes recall to 98 % with a precision collapse to 0.19, raising expected cost back to 0.732.
* **Oversampling** repeats minority-class rows to reach **32 710 training rows** (50/50). Results at threshold 0.5 are nearly identical to undersampling (recall 61 %, expected cost 0.610), confirming both strategies impose a similar effective prior shift. Oversampling preserves all original majority-class information, giving it a marginally higher AUC (0.741 vs 0.742) but no practical difference at this scale.
* **SMOTE** (Synthetic Minority Over-sampling Technique) generates new synthetic minority-class observations by interpolating between existing ones in feature space, rather than simply duplicating rows. It produces the same 32 710-row balanced training set as oversampling. At threshold 0.5 the results are nearly identical to plain oversampling (recall **61 %**, F1 **0.499**, expected cost **0.616**), with SMOTE marginally worse, likely because LDA's linear boundary cannot exploit the smoother minority-class region that SMOTE creates. The benefit of SMOTE is more pronounced for non-linear models. At the cost-optimal threshold (0.20) it similarly overshoots to around 98 % recall with a precision collapse (expected cost 0.727).

---

## 7.6 Overall Comparison

```{python}
all_rows = []

configs = [
    ('LDA: default threshold (0.50)',       y_pred_lda,        y_prob_lda),
    (f'LDA: cost threshold ({opt_t:.2f})',  y_pred_lda_tuned,  y_prob_lda),
    ("LDA: Youden's J threshold",           y_pred_lda_youden, y_prob_lda),
    ('LDA: balanced priors (0.5/0.5)',      lda_bal.predict(X_test_scaled),
                                              lda_bal.predict_proba(X_test_scaled)[:, 1]),
    ('LDA: oversample train',               (lda_up.predict_proba(X_te_np)[:, 1] >= 0.5).astype(int),
                                              lda_up.predict_proba(X_te_np)[:, 1]),
    ('LDA: SMOTE train',                    (lda_smote.predict_proba(X_te_np)[:, 1] >= 0.5).astype(int),
                                              lda_smote.predict_proba(X_te_np)[:, 1]),
    ('GNB: default threshold (0.50)',       y_pred_gnb,        y_prob_gnb),
    (f'GNB: cost threshold ({opt_t:.2f})',  (y_prob_gnb >= opt_t).astype(int), y_prob_gnb),
]

for name, yp, yprob in configs:
    all_rows.append({
        'Configuration':      name,
        'Accuracy':           accuracy_score(y_test, yp),
        'Recall (Default)':   recall_score(y_test, yp),
        'Precision (Default)':precision_score(y_test, yp, zero_division=0),
        'F1 (Default)':       f1_score(y_test, yp),
        'ROC-AUC':            roc_auc_score(y_test, yprob),
        'Expected Cost':      expected_cost(y_test, yp, C),
    })

final_df = pd.DataFrame(all_rows).set_index('Configuration').round(4)
display(final_df)
```

### Key takeaways

* **Threshold tuning is the highest-leverage, zero-retraining intervention**: shifting LDA's threshold from 0.50 to **0.200** (grid-optimal; theoretical $p^* = 0.167$) raises default recall from 27 % to **63 %**, F1 from 0.38 to **0.50**, and delivers the **lowest expected cost of 0.599** across all configurations tested.
* **Youden's J statistic** offers a symmetrical, ROC-curve-based alternative: it selects threshold **0.2244** (J = 0.398), maximising $\text{TPR} - \text{FPR}$ with sensitivity 0.60 and specificity 0.80. This yields recall 60 %, F1 **0.519**, and expected cost **0.600**, nearly indistinguishable from the cost-matrix threshold (0.200, cost 0.599) in this dataset. The two criteria converge here because the 5:1 cost ratio implies a relatively mild shift from 0.5; in scenarios with more extreme asymmetry (e.g. 20:1) the gap would widen substantially, and the cost-matrix threshold would be clearly preferable.
* **Balanced priors** are the best single model-level fix: one parameter change raises recall to **60 %** and F1 to **0.517** (the highest of any configuration) with expected cost of 0.601, essentially tied with threshold tuning and more principled theoretically.
* **SMOTE** produces the same 32 710-row balanced training set as random oversampling but fills the minority-class region with synthetic interpolated points rather than duplicates. At threshold 0.5 it achieves recall **61 %**, F1 **0.499**, expected cost **0.616**, marginally worse than plain oversampling (F1 0.507, cost 0.610) because LDA's linear boundary cannot exploit the smoother feature-space coverage. SMOTE's advantage becomes more pronounced with non-linear models.
* **Resampling at threshold 0.5** brings comparable gains to balanced priors (recall around 61 %, expected cost around 0.61), but combining resampling with the cost-optimal threshold backfires: the model overshoots to around 98 % recall with precision of 0.19, worsening expected cost to 0.73.
* **GNB with cost threshold (0.20)** achieves a recall of **59 %** and expected cost of **0.637**, consistently behind the best LDA variants. GNB's miscalibrated probabilities make threshold selection less precise.
* **ROC-AUC is invariant across all interventions** (all LDA variants: 0.740–0.741): every strategy improves decision rules without changing the underlying ranking ability of the model.
* The expected-cost framework is portable: with a 5:1 FN/FP ratio the optimal threshold is 0.167; a bank with a different loss estimate simply updates $c_{FN}$ and recomputes $p^* = c_{FP}/(c_{FP}+c_{FN})$.

# 8. k-Nearest Neighbours (k-NN)

## 8.1 Theory

k-Nearest Neighbours (k-NN) is a distance-based, non-parametric classifier. For a new observation, it:

1. Computes its distance to every training point (here we use Euclidean distance).
2. Selects the k closest points (the “neighbours”).
3. Predicts the class by majority vote among those neighbours.

Because k-NN relies directly on distances, it must be applied to standardised features. We therefore use the standardised design matrices `X_train_scaled` and `X_test_scaled` from earlier sections.

The main tuning choice is k:

* Small k → more flexible boundary (higher variance).
* Large k → smoother boundary (higher bias).

## 8.2 Fit & Evaluate (baseline k = 5)

```{python}
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import PrecisionRecallDisplay, average_precision_score

knn_5 = KNeighborsClassifier(n_neighbors=5)   # uniform vote (default)
knn_5.fit(X_train_scaled, y_train)

y_pred_knn5 = knn_5.predict(X_test_scaled)
y_prob_knn5 = knn_5.predict_proba(X_test_scaled)[:, 1]
auc_knn5    = roc_auc_score(y_test, y_prob_knn5)

print("k-NN (k=5): Classification Report")
print(classification_report(y_test, y_pred_knn5, target_names=['No Default', 'Default']))
print(f"ROC-AUC: {auc_knn5:.4f}")

fig, axes = plt.subplots(1, 2, figsize=(13, 4))

ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_knn5),
                       display_labels=['No Default', 'Default']).plot(ax=axes[0], colorbar=False)
axes[0].set_title('k-NN (k=5) - Confusion Matrix')

RocCurveDisplay.from_predictions(y_test, y_prob_knn5, ax=axes[1],
                                 name=f'k-NN k=5 (AUC = {auc_knn5:.3f})')
axes[1].plot([0, 1], [0, 1], 'k--', lw=1)
axes[1].set_title('k-NN (k=5) - ROC Curve')
plt.tight_layout()
plt.show()


ap_knn5 = average_precision_score(y_test, y_prob_knn5)
fig, ax = plt.subplots(figsize=(6.5, 4))
PrecisionRecallDisplay.from_predictions(y_test, y_prob_knn5, ax=ax,
                                        name=f'k-NN k=5 (AP = {ap_knn5:.3f})')
ax.set_title("k-NN (k=5) - Precision–Recall Curve")
plt.tight_layout()
plt.show()
```


### Interpretation

* With \(k=5\), k-NN achieves 80 % accuracy on the test set, but (as in earlier models) accuracy is inflated by class imbalance (only about 22 % of the test set are defaulters).
* From the confusion matrix (TN = 6450, FP = 559, FN = 1243, TP = 748), the model is conservative in predicting “Default”: it predicts default for 1307 / 9000 = 14.5 % of cases, below the true default rate (22.1 %).
* The recall on the Default class is 0.38, meaning k-NN correctly identifies 748 defaulters but still misses 1243 (about 62 % of actual defaulters). This is the key weakness at the standard 0.50 threshold.
* On the other hand, the model performs well on the majority class: No Default recall is 0.92, i.e. only about 8 % of non-defaulters are incorrectly marked as default (FP = 559 out of 7009).
* The ROC-AUC of 0.7158 indicates meaningful ranking ability (better than random), but thresholding at 0.50 leaves many false negatives. This motivates (i) tuning \(k\) via cross-validation (Section 8.3).
* The Precision–Recall curve summarises the trade-off on the minority class across thresholds. The average precision (AP) is 0.422, which indicates moderate ability to retrieve defaulters, but also shows that precision drops quickly as recall increases (i.e. capturing more defaulters requires accepting many more false positives).



## 8.3 Selecting k by Cross-Validation

To choose \(k\) systematically, we evaluate a grid of odd \(k\) values using 5-fold stratified cross-validation, selecting the \(k\) that maximises mean ROC-AUC on the training set.


```{python}
from sklearn.model_selection import StratifiedKFold

k_grid = list(range(1, 42, 2))   # odd ks reduce ties in binary voting
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

rows = []
for k in k_grid:
    aucs = []
    for tr_idx, va_idx in cv.split(X_train_scaled, y_train):
        X_tr, X_va = X_train_scaled.iloc[tr_idx], X_train_scaled.iloc[va_idx]
        y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]

        m = KNeighborsClassifier(n_neighbors=k)
        m.fit(X_tr, y_tr)
        p = m.predict_proba(X_va)[:, 1]
        aucs.append(roc_auc_score(y_va, p))

    rows.append({
        "k": k,
        "cv_auc_mean": float(np.mean(aucs)),
        "cv_auc_sd": float(np.std(aucs))
    })

knn_cv_df = pd.DataFrame(rows).sort_values("cv_auc_mean", ascending=False)
display(knn_cv_df.head(10))

best_k = int(knn_cv_df.iloc[0]["k"])
print(f"Best k by mean CV AUC: {best_k}")

fig, ax = plt.subplots(figsize=(8, 4))
tmp = knn_cv_df.sort_values("k")
ax.plot(tmp["k"], tmp["cv_auc_mean"], marker="o")
ax.set_title("k-NN: Cross-validated ROC-AUC vs k")
ax.set_xlabel("k")
ax.set_ylabel("Mean CV ROC-AUC")
plt.tight_layout()
plt.show()

from sklearn.metrics import accuracy_score
from sklearn.model_selection import StratifiedKFold
import numpy as np
import pandas as pd

k_grid = list(range(1, 42, 2))
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

rows_acc = []
for k in k_grid:
    accs = []
    for tr_idx, va_idx in cv.split(X_train_scaled, y_train):
        X_tr, X_va = X_train_scaled.iloc[tr_idx], X_train_scaled.iloc[va_idx]
        y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]

        m = KNeighborsClassifier(n_neighbors=k)
        m.fit(X_tr, y_tr)
        y_hat = m.predict(X_va)
        accs.append(accuracy_score(y_va, y_hat))

    rows_acc.append({
        "k": k,
        "cv_acc_mean": float(np.mean(accs)),
        "cv_acc_sd": float(np.std(accs))
    })

knn_cv_acc_df = pd.DataFrame(rows_acc).sort_values("k")

fig, ax = plt.subplots(figsize=(8, 4))
ax.plot(knn_cv_acc_df["k"], knn_cv_acc_df["cv_acc_mean"], marker="o")
ax.set_title("k-NN: Cross-validated Accuracy vs k")
ax.set_xlabel("k")
ax.set_ylabel("Mean CV Accuracy")
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
tmp = knn_cv_df.sort_values("k").copy()

fig, ax = plt.subplots(figsize=(8, 4))
ax.plot(tmp["k"], tmp["cv_auc_mean"], marker="o")

lower = tmp["cv_auc_mean"] - tmp["cv_auc_sd"]
upper = tmp["cv_auc_mean"] + tmp["cv_auc_sd"]
ax.fill_between(tmp["k"], lower, upper, alpha=0.2)

ax.set_title("k-NN: Cross-validated ROC-AUC vs k (mean ± 1 sd)")
ax.set_xlabel("k")
ax.set_ylabel("Mean CV ROC-AUC")
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()


```

### Interpretation

* The cross-validation results show a clear bias–variance trade-off. For very small \(k\) (e.g. \(k=1\)), mean CV ROC-AUC is low (about 0.61), which is consistent with high-variance behaviour (the classifier is too sensitive to local noise).
* As \(k\) increases, the mean CV ROC-AUC improves rapidly at first (by around \(k \approx 5\) it is already about 0.71), then increases more gradually and plateaus for larger \(k\).
* The CV Accuracy curve shows a similar pattern: accuracy rises quickly for small \(k\) and then stabilises around 0.80–0.81 for moderate to large \(k\). This reinforces that increasing \(k\) beyond roughly \(20\) gives only marginal improvements in overall classification accuracy.
* The shaded band (mean ± 1 sd) indicates that variability across folds is relatively small once \(k\) is moderate, and the curves for large \(k\) overlap substantially. This suggests that performance is stable in the \(k \approx 30\)–\(41\) region, so choosing \(k=41\) is reasonable but not uniquely optimal (nearby values would perform very similarly).
* The curve flattens noticeably after roughly \(k \approx 20\), where improvements become marginal. This suggests that once the neighbourhood is large enough, adding more neighbours produces only small gains in ranking performance.
* The best mean CV ROC-AUC occurs at \(k = 41\) with mean CV ROC-AUC 0.7609 and standard deviation about 0.0091. Nearby values (e.g. \(k=39\), \(k=37\)) are extremely close, indicating that performance is fairly stable in this region.
* We take \(k=41\) forward into the final test evaluation (Section 8.4).


## 8.4 Fit & Evaluate Best k-NN on the Test Set

```{python}
knn_best = KNeighborsClassifier(n_neighbors=best_k)
knn_best.fit(X_train_scaled, y_train)

y_pred_knn = knn_best.predict(X_test_scaled)
y_prob_knn = knn_best.predict_proba(X_test_scaled)[:, 1]
auc_knn    = roc_auc_score(y_test, y_prob_knn)

print(f"k-NN (k={best_k}): Classification Report")
print(classification_report(y_test, y_pred_knn, target_names=['No Default', 'Default']))
print(f"ROC-AUC: {auc_knn:.4f}")

fig, axes = plt.subplots(1, 2, figsize=(13, 4))

ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_knn),
                       display_labels=['No Default', 'Default']).plot(ax=axes[0], colorbar=False)
axes[0].set_title(f'k-NN (k={best_k}) - Confusion Matrix')

RocCurveDisplay.from_predictions(y_test, y_prob_knn, ax=axes[1],
                                 name=f'k-NN (AUC = {auc_knn:.3f})')
axes[1].plot([0, 1], [0, 1], 'k--', lw=1)
axes[1].set_title(f'k-NN (k={best_k}) - ROC Curve')
plt.tight_layout()
plt.show()


from sklearn.metrics import PrecisionRecallDisplay, average_precision_score

ap_knn5 = average_precision_score(y_test, y_prob_knn5)
ap_knn  = average_precision_score(y_test, y_prob_knn)

fig, axes = plt.subplots(1, 2, figsize=(13, 4))

RocCurveDisplay.from_predictions(y_test, y_prob_knn5, ax=axes[0],
                                 name=f'k-NN k=5 (AUC = {auc_knn5:.3f})')
RocCurveDisplay.from_predictions(y_test, y_prob_knn, ax=axes[0],
                                 name=f'k-NN k={best_k} (AUC = {auc_knn:.3f})')
axes[0].plot([0, 1], [0, 1], 'k--', lw=1)
axes[0].set_title("ROC: baseline vs tuned")

PrecisionRecallDisplay.from_predictions(y_test, y_prob_knn5, ax=axes[1],
                                        name=f'k-NN k=5 (AP = {ap_knn5:.3f})')
PrecisionRecallDisplay.from_predictions(y_test, y_prob_knn, ax=axes[1],
                                        name=f'k-NN k={best_k} (AP = {ap_knn:.3f})')
axes[1].set_title("Precision–Recall: baseline vs tuned")

plt.tight_layout()
plt.show()

```

### Interpretation

* The cross-validated choice \(k=41\) generalises well: the mean CV ROC-AUC was 0.7609 and the test ROC-AUC is 0.7569, a small drop of about 0.004, suggesting limited overfitting.
* Relative to the baseline \(k=5\) (ROC-AUC 0.7158), the tuned model improves ranking performance by about 0.041 in ROC-AUC and slightly increases accuracy (0.81 vs 0.80). This confirms that tuning \(k\) materially improves discrimination.
* From the confusion matrix (TN = 6679, FP = 330, FN = 1402, TP = 589), the model is even more conservative at the default 0.50 threshold: it predicts “Default” only 919 / 9000 = 10.2% of the time, well below the true default rate (22.1%).
* For the Default class, precision increases to 0.64 (fewer false alarms), but recall drops to 0.30 (it misses 1402 / 1991 \(\approx\) 70% of defaulters). In other words, increasing \(k\) smooths the decision rule and reduces false positives, but increases false negatives.
* Performance on the majority class remains strong (No Default recall 0.95), which helps overall accuracy but again highlights that accuracy alone is not sufficient under class imbalance. 
* The combined ROC plot shows that the tuned model dominates the baseline across most thresholds: AUC increases from 0.716 (k=5) to 0.757 (k=41), confirming that tuning \(k\) improves the model’s ability to rank defaulters ahead of non-defaulters.
* The Precision–Recall plot provides a clearer view under class imbalance. Average precision increases from 0.422 (k=5) to 0.514 (k=41), meaning the tuned model achieves better precision at a given recall level over a wide range of thresholds. This improvement in ranking contrasts with the lower Default recall at the fixed 0.50 threshold, highlighting that the main limitation is the chosen decision threshold rather than the model’s probability ordering.


# 9. Conclusion

This report analysed the UCI Default of Credit Card Clients dataset (30,000 cardholders) with the goal of predicting whether a client defaults on their October 2005 payment. The initial EDA showed a moderate class imbalance (about 78% no default vs 22% default), implying that accuracy alone can be misleading if a model mostly predicts the majority class. 

A structured preprocessing pipeline was essential before modelling. We replaced any undocumented category values with a single “Other” group, and then one-hot encoded the categorical variables so the model could use them consistently. To reduce the strong right-skew in monetary variables, a selective log1p transform was applied to 13 skewed columns (credit limit, bills, and payments), and then all features were standardised using a scaler fit only on the training set to avoid leakage. 

For probabilistic classifiers, the results show a clear trade-off between overall accuracy and minority-class detection. LDA achieved the best overall ranking among the generative models (ROC-AUC \approx 0.7401, accuracy \approx 0.8066) but was conservative, with Default recall \approx 0.2697 at threshold 0.50. QDA improved Default recall (\approx 0.4902) at the expense of lower ROC-AUC (\approx 0.7151) and lower accuracy (\approx 0.7520), consistent with the added flexibility being limited by correlated billing features. Gaussian Naïve Bayes offered the best minority-class balance among these three (Default recall \approx 0.5113, Default F1 \approx 0.4891) while maintaining an ROC-AUC comparable to LDA (\approx 0.7396). 

Because the application is credit risk, Section 7 demonstrated that performance should be evaluated under asymmetric error costs. Using a cost matrix with false negatives more expensive than false positives yields a theoretical cost-optimal probability threshold of \(p^* \approx 0.1667\).  The highest-leverage intervention was threshold tuning on LDA probabilities: moving from the default threshold 0.50 (expected cost \approx 0.8397) to a cost-aligned threshold around 0.20 reduced expected cost to about 0.5990 and raised Default recall to about 0.6339.  Balanced priors provided a similarly strong, model-level alternative with expected cost around 0.601, while resampling (undersampling/oversampling/SMOTE) improved recall at threshold 0.50 but could overshoot under the cost-optimal threshold, worsening expected cost. 

Finally, k-NN showed that non-parametric models can improve discrimination when tuned: cross-validation selected \(k=41\), and test ROC-AUC increased to 0.7569 (vs 0.7158 for \(k=5\)).  However, at the default 0.50 threshold the tuned k-NN remained conservative (Default recall \approx 0.30), reinforcing the broader finding that ranking performance (ROC-AUC) and decision performance (cost-sensitive classification) are not the same objective.

**Overall recommendation:** for an operational decision rule under the stated 5:1 cost asymmetry, the strongest and simplest solution in this report is to use LDA probability outputs with a cost-sensitive threshold (\approx 0.20) (or equivalently balanced priors), because it delivers the lowest expected cost while substantially increasing Default recall.
